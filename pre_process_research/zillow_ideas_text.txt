
######################################
GOAL OF PROJECT
######################################

------------
Will:  we predict the error, they subtract it from their model and it is like error correction to make the model better.
Metric used:  mean absolute squared error:  absolute value of the differece between prediction and actual log-error
------------

In this competition, Zillow is asking you to predict the log-error between their Zestimate and the actual sale price, given all the features of a home. 
The log error is defined as

logerror=log(Zestimate)−log(SalePrice)

You are asked to predict 6 time points for all properties: October 2016 (201610), November 2016 (201611), December 2016 (201612), October 2017 (201710), November 2017 (201711), and December 2017 (201712).  Data is from 2016 for these categories:  (Los Angeles, Orange and Ventura, California).

Then on another page:

In the qualifying round, you’ll be building a model to improve the Zestimate residual error. In the final round, you’ll build a home valuation algorithm from the ground up, using external data sources to help engineer new features that give your model an edge over the competition.

https://www.kaggle.com/c/zillow-prize-1#evaluation

For each property (unique parcelid), you must predict a log error for each time point. You should be predicting 6 timepoints: October 2016 (201610), November 2016 (201611), December 2016 (201612), October 2017 (201710), November 2017 (201711), and December 2017 (201712).

#######################################
Helpful Background Research and Links
#######################################

## EDA Coding example as Markdown
- https://www.kaggle.com/philippsp/exploratory-analysis-zillow/code

## workflow coding example (regression)
- https://www.kaggle.com/monkeyorman/simple-linear-regression-in-r-0-0648835/code

## recommendation from graduate of April Cohort:
- https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-zillow-prize

## other links to discussions and posts on zillo competition
- https://www.kaggle.com/c/zillow-prize-1/discussion/34168

##########################################################
Ideas for General Approach and Endgame Strategies
##########################################################

##### ideas: missingness strategies #############
1) Knn for categorical data
2) regression for correlated values (linear)

### ideas:  strategies for endgame ######
Strategy one:
---------------
1) divide data into 3 groups - one for each target month we are to predict on: 201610, 201611, 201612
   - note that sample of what our upload file will look like has 6 columns like these where we fill in first 3
   - last 3 are for round 2 that I think starts in October
2) model for earliest time period is based on logerror w/ min date associated to it
3) model for latest time period is based on logerror w/ max date associated to it
4) model for in-between date gets value associated to mean or median date as target for prediction

=======================================
Missingness - Specific Fields
=======================================
1) from Shu's revised code as of 8/12 (in comments): "e.g., use latitude and longitude to impute censustract etc.""
########### addtional fields (not Mitch's set) #########
# rawcensustrackandblock, censustrackandblock
#  * look like different numbers
#  * guy at zillow says they are same data
#  * censutrackandblock has more data and is integer versus string
# censustract: idea from Shu - use latitude and longitude to impute censustract etc.


###################################
Fix Properties (and Data) in Time?
###################################

Fields that relate:
-------------------

Listed In Data Dictionary:
1) yearbuilt
2) assessmentyear - year tax assessment is from
3) taxdelinquencyflag - Property taxes for this parcel are past due as of 2015
4) taxdelinquencyyear - Year for which the unpaid propert taxes were due 

Other Time-Based Fields:
------------------------
train_2016_v2.csv:
  transactiondate


#################################################
Location
#################################################
regionzipid <> true zip code (has been obfuscated)

#################################################
Initial Thoughts and ideas
#################################################

1) cor plot of all fields (w/o missing values) to eliminate fields that seem to have no relation to target variable
2) initial testing done with simple models on fields w/o missing values again looking for things that seem to have no correlation to target
3) variables with high correlation could then become areas of focus for cleanup and the final model
4) Tom suggests doing a baseline test where we expect zestimate to be perfect, submit and then see how to adjust?
5) Log Error histogram shows distorted pattern ... other kagglers tried removing outliers to make it look normal
   - what if it looks that way because it was transformed?  What would it look like if transormed back to original error? (something to check)
   - if this theory is right, maybe we predict the log and transform it to logerror?

#################################
Assignments / Schedule
#################################
Initial Missingness:
--------------------
Mitch working on: 19 - 36 except:  22-26
All members taking about a 4th to work on

------------------------
Initial Schedule ... To Be Adjusted:
------------------------
8/11: Clean Columns
8/12 - 8/13 (Sat/Sun):  EDA, Regression Factors, etc.
8/14:  targeted to begin ML Prep
8/19:  First Submission on Kaggle
8/21:  Looks like last day to work on it
8/22:  Kagel project due (reiminder first thing in morning on 22nd)
       - looks like project is expected to be done here so we can work on our projects
8/23:  Kagel presentations day


################################################
Random Code Draft Snippets
################################################
## Get Files - replaced in live code - preserving original
# -----------
properties <- fread('./input/properties_2016.csv', header=TRUE, na.strings=c("", " ", "NA", "N/A", "null"), blank.lines.skip = TRUE,
                    colClasses=list(character=50))  ## eliminates warning so we know nothing else is wrong
                    ## na.string = common values for NA that may have been used
dim(properties)

train = fread('train_2016_v2.csv')
full = properties %>% left_join(., train, by = 'parcelid')

####################################################
8/16/2017 Notes
####################################################
Issues with factors:
--------------------
Latitude
Longitude

multi linear submitted
3 more models

then could do xgboost

-------------------------------------
## will's changes as of 8/16/2017:
-------------------------------------

full$fips=as.factor(full$fips)
full$latitude = as.numeric(full$latitude)
full$longitude = as.numeric(full$longitude)
full$numberofstories = as.factor(full$numberofstories)
full$propertycountylandusecode = as.factor(full$propertycountylandusecode) 
## but propertycountylandusecod sucks and should be avoided

censustrackandblock
  - fips + track# + block$ (track is 4 digits)

fireplaceflag ## field is fine

fullbathroomcnt => for linear regression make this numeric
hottuborspa => binary
heatingorsystemtypeid = 13 used to impute nas because 13 = "none" in data dictionary
  * making this 2 or not 2 which means central or not central

10+ factors:  terrible p-values and ran slow (less meaningful coefficients)

##############################
merged DF
##############################
* Early models trained based on a mean logerror by date probably without a date column
* this is what we call "full"
* do we want different strategies with date field as predictive?
* how would we use what we have to get where i want to go?

Random things to look into (time permitting):
-------------------------------------------
glm() ... for multi logistical regression?

ideas:
- knn crashes - puts too much in memory up front?
  - could we sample the data in 100K record sets
  - do Knn, then sample from that and do knn on top
    - if we do this, do we compound errors or compound information from past knn border into new one?
    - to do this:  need to preserve index as well as values in random sets so we get all data
    -              and so we know what and how to bring data back together for the answers?
  - Might not need to use all data to do this ... predict goes in small sets so as not to crash
  - based on what the model did on smaller data

------------------------
Clustering as a solution?
--------------------------
could we take bad factors and cluster them down to a small number of solutions that could then be averaged for an answer?

Bad Idea: recorded for future thought:
Could we convert a factor to numeric values - one for each factor ... and use PCA to reduce?  
 - Would it help or hurt predictability?
 - How would you explain what got combined after you did it?
 - lecture seems to imply this would not work ... (early in lecture)
 - later in lecture: based on how this works, converting columns to numbers to feed them in would lose all meaning (late in lecture)
 - this would require some very complex thought about how to convert columns to meaningful numbers that could be parsed.

Weird idea:  
- get frequency counts of certain categories grouped by some location factor
- is there a way to model to figure out which frequncies are important and then use in yet another model?
- does PCA help here?  (just random unformed thoughts to be considered later)

------------------------------
Notes for linear Relationships
-------------------------------
strong correlation:
lm1 used in cleaning/imputation:  90% r^2, model: lm(formula = taxamount ~ taxvaluedollarcnt, data = full)

#######################
Shu Jumpstart Python
#######################
* was going to put notes here ... forget what they are now ...
